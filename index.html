<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in MLLMs - Oi et al.">
  <meta name="description" content="HATCH: A training framework for multi-image spatial reasoning in MLLMs using cross-view correspondence and stepwise viewpoint transformation.">
  <meta name="keywords" content="multimodal large language models, spatial reasoning, cross-view correspondence, viewpoint transformation, machine learning, computer vision, AI">
  <meta name="author" content="Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Institute of Science Tokyo">
  <meta property="og:title" content="From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in MLLMs">
  <meta property="og:description" content="HATCH: A training framework for multi-image spatial reasoning in MLLMs using cross-view correspondence and stepwise viewpoint transformation.">
  <meta property="og:url" content="https://arxiv.org/abs/2602.08735">
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/teaser.jpg">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="HATCH - Research Preview">
  <meta property="article:published_time" content="2026-02-09T00:00:00.000Z">
  <meta property="article:author" content="Masanari Oi">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="multimodal large language models">
  <meta property="article:tag" content="spatial reasoning">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:title" content="From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in MLLMs">
  <meta name="twitter:description" content="HATCH: A training framework for multi-image spatial reasoning in MLLMs using cross-view correspondence and stepwise viewpoint transformation.">
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/teaser.jpg">
  <meta name="twitter:image:alt" content="HATCH - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models">
  <meta name="citation_author" content="Oi, Masanari">
  <meta name="citation_author" content="Maeda, Koki">
  <meta name="citation_author" content="Koike, Ryuto">
  <meta name="citation_author" content="Oba, Daisuke">
  <meta name="citation_author" content="Inoue, Nakamasa">
  <meta name="citation_author" content="Okazaki, Naoaki">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="arXiv">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2602.08735.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models - Oi et al. | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/hatch.png">
  <link rel="apple-touch-icon" href="static/images/hatch.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models",
    "description": "HATCH: A training framework for multi-image spatial reasoning in MLLMs using cross-view correspondence and stepwise viewpoint transformation.",
    "author": [
      {
        "@type": "Person",
        "name": "Masanari Oi",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Science Tokyo"
        }
      },
      {
        "@type": "Person",
        "name": "Koki Maeda",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Science Tokyo"
        }
      },
      {
        "@type": "Person",
        "name": "Ryuto Koike",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Science Tokyo"
        }
      },
      {
        "@type": "Person",
        "name": "Daisuke Oba",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Science Tokyo"
        }
      },
      {
        "@type": "Person",
        "name": "Nakamasa Inoue",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Science Tokyo"
        }
      },
      {
        "@type": "Person",
        "name": "Naoaki Okazaki",
        "affiliation": [
          {
            "@type": "Organization",
            "name": "Institute of Science Tokyo"
          },
        ]
      }
    ],
    "datePublished": "2026-02-09",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://arxiv.org/abs/2602.08735",
    "image": "https://YOUR_DOMAIN.com/static/images/teaser.jpg",
    "keywords": ["multimodal large language models", "spatial reasoning", "cross-view correspondence", "viewpoint transformation", "machine learning", "computer vision"],
    "abstract": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://arxiv.org/abs/2602.08735"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Multimodal Large Language Models"
      },
      {
        "@type": "Thing",
        "name": "Spatial Reasoning"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Institute of Science Tokyo",
    "url": "https://www.isct.ac.jp/",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!--
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 1</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://sites.google.com/view/masanarioi/" target="_blank">Masanari Oi</a>,</span>
                <span class="author-block">
                  <a href="https://silviase.com/" target="_blank">Koki Maeda</a>,</span>
                  <span class="author-block">
                    <a href="https://ryuryukke.github.io/" target="_blank">Ryuto Koike</a>,</span>
                  <span class="author-block">
                    <a href="https://daioba.github.io/" target="_blank">Daisuke Oba</a>,</span>
                  <span class="author-block">
                    <a href="https://mmai.tech/" target="_blank">Nakamasa Inoue</a>,</span>
                  <span class="author-block">
                    <a href="https://www.chokkan.org/index.en.html" target="_blank">Naoaki Okazaki</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      Institute of Science Tokyo
                      <br>arXiv 2026
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                <!-- arXiv -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2602.08735" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- GitHub Code - Coming soon -->
                  <span class="link-block">
                    <a href="#" class="external-link button is-normal is-rounded is-dark" disabled style="pointer-events: none; opacity: 0.5;">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.jpg" alt="HATCH Overview" style="width: 100%; height: auto;" loading="eager">
      <h2 class="subtitle has-text-centered">
        <strong>H</strong>uman-<strong>A</strong>ware <strong>T</strong>raining for <strong>C</strong>ross-view correspondence and viewpoint c<strong>H</strong>ange (HATCH), a training framework for multi-image spatial reasoning in multimodal large language models.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Results section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Results</h2>
      <div class="columns is-centered">
        <div class="column is-9">
          <div class="content">
            <img src="static/images/mv_results.png" alt="Performance on multi-image spatial reasoning benchmarks" loading="lazy" style="width: 100%; height: auto;"/>
            <p class="has-text-centered" style="margin-top: 1rem;">
              <strong>Performance on multi-image spatial reasoning benchmarks</strong>
            </p>
          </div>
        </div>
        <div class="column is-5">
          <div class="content">
            <img src="static/images/si_results.png" alt="Performance on single-image spatial reasoning benchmarks" loading="lazy" style="width: 100%; height: auto;"/>
            <p class="has-text-centered" style="margin-top: 1rem;">
              <strong>Performance on single-image spatial reasoning benchmarks</strong>
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End results section -->




<!-- Youtube video -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<!-- End youtube video -->


<!-- Video carousel -->
<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<!-- End video carousel -->






<!-- Paper poster -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/poster.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section>
-->
<!--End paper poster -->



<!--BibTex citation -->
<!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{oi2026correspondenceactionshumanlikemultiimage,
  title={From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models},
  author={Masanari Oi and Koki Maeda and Ryuto Koike and Daisuke Oba and Nakamasa Inoue and Naoaki Okazaki},
  year={2026},
  eprint={2602.08735},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2602.08735}
}</code></pre>
    </div>
</section>
-->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
